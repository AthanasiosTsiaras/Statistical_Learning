---
title: "Machine_Learning_Assignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.Overview
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

# 2.Data Processing
We get the data from the .csv files. The training set is separated in two sets, one for
testing and one for training. The testing set is going to be used only at the quiz.

```{r, echo=TRUE}
library(caret)
testing <- read.csv("pml-testing.csv", header = TRUE)
training <- read.csv("pml-training.csv", header = TRUE)

inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
Train <- training[inTrain, ]
Test  <- training[-inTrain, ]
```
The datasets contain 160 columns. We check which columns have identifier variables that are of no use for analysis,
which columns are empty and which columns are more than 95% missing values. We delete all of those and we are left with
54 columns

```{r, echo=TRUE}
empty_cols <- nearZeroVar(Train)
Train <- Train[,-empty_cols]
Test <- Test[,-empty_cols]

near_zero <- function(x){
        nas = 0
        for (i in 1:length(x)){
                if (is.na(x[i]==TRUE)){
                nas = nas+1
                
                }
        }
        if (nas > 0.95*length(col)){
                return(TRUE)
                
        }else{
                return(FALSE)
        }
}

Train <- Train[,!apply(Train,2,near_zero)]
Test <- Test[,!apply(Test,2,near_zero)]

# Removing identifier variables
Train <- Train[,-(1:5)]
Test <- Test[,-(1:5)]
```

# 3.Model construction and validation
We make 3 models, one with the random forest method, one with the boosting method and one with the decision trees method.
We keep the random forest one since it's the most accurate. We use it to predict the testing dataset and the prediction
results are shown. These are the results used for the quiz.

```{r, echo=TRUE}
mod1 <- train(classe~., data=Train, method="rf")
pred1 <- predict(mod1, Test)
rf_accu <- confusionMatrix(pred1, Test$classe)$overall[1]

mod2 <- train(classe~., data=Train, method="gbm", verbose=FALSE)
pred2 <- predict(mod2, Test)
gbm_accu <- confusionMatrix(pred2, Test$classe)$overall[1]

mod3 <- train(classe~., data=Train, method="rpart")
pred3 <- predict(mod3, Test)
rpart_accu <- confusionMatrix(pred3, Test$classe)$overall[1]

accu_compar<- data.frame(rf_accu=rf_accu,gbm_accu=gbm_accu, rpart_accu=rpart_accu)
print(accu_compar)

pred_final <- predict(mod1,testing)
print(pred_final)
```

